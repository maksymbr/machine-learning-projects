\section{Code implementation and testing}
\label{sec:code_imp}


In this section I describe the various parts of the code I've written. It equally works for "fake" (manually generated) and "real" (Norwegian terrain) data sets. The whole folder structure can be viewed as follows:
\begin{itemize}
    \item reglib.py - small library, which contains all necessary methods to calculate $\beta$'s, MSE, CrossValidation and Bias-Variance trade-off;
    \item regression.py - main module, which contains the entry point of the program together with calling procedures for the reglib.py library;
\end{itemize}

\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\begin{center}
\begin{tikzpicture}[
  grow via three points={one child at (0.5, -0.7) and
  two children at (0.5, -0.7) and (0.5,-1.4)},
  edge from parent path={[->](\tikzparentnode.south) |- (\tikzchildnode.west)}]
    \node {root}
    child { node {regression.py}}		
    child { node {reglib.py}}
    child { node {Data}
        child { node {$\mathrm{SRTM\_data\_Norway\_1.tif}$} }
        child { node {$\mathrm{SRTM\_data\_Norway\_2.tif}$} }
    }
    % adding these to make some space
    child [missing] {}				
    child [missing] {}				
    child { node {Output}
        child { node {...} }
    };
\end{tikzpicture}
\end{center}
\begin{itemize}
    \item \textbf{reglib.py} - small library, which contains all necessary methods to calculate $\beta$'s, MSE, CrossValidation and Bias-Variance trade-off;
    \item \textbf{regression.py}  - main module, which contains the entry point of the program together with calling procedures for the reglib.py library;
    \item \textbf{Data} - folder which contains the real data. Note: you do not necessarily need to copy the data files inside, but can use the full path in your OS;
    \item \textbf{Output} - contains all the output files produced during the program run.
\end{itemize}

To obtain the program please navigate to the directory you want the program to be stored at with:
\begin{lstlisting}
cd /path/to/your/directory/
\end{lstlisting}
and simply clone the repository by typing:
\begin{lstlisting}
git clone https://github.com/maksymbr/machine-learning-projects.git
\end{lstlisting}

The script was written and tested with python 3.7. I was running it either through JupyterNotebook, Spider or PyCharm. But, it is possible to run it through terminal. Just locate to the directory with the script and type:
\begin{lstlisting}
python regression.py
\end{lstlisting}

Before you run it make sure you have installed such libraries as: \textbf{numpy}, \textbf{sympy}, \textbf{scipy}, \textbf{joblib}, \textbf{itertools}, \textbf{matplotlib}.

After execution, you will be asked some questions about the data set and other parameters you want to use. In my opinion, it is quite simple to understand, so I will omit the tedious explanations here and will go straight to the code explanation.

\subsection{Fake Data}

The original idea was to write a program which, in principle, will be able to work with any amount of independent variables. For this I am using a sympy library to generate the \textit{symbolic} array of independent variables as:
\begin{lstlisting}
x_symb = sp.symarray('x', n_vars, real = True)
\end{lstlisting}
which will result in a list of variables $x_0$, $x_1$, $x_2$, ... $x_n$, where $n=\mathrm{n\_vars}$.

For a given set of points, I am generating a set of values for the symbolic variables as follows:
\begin{lstlisting}
x_vals = x_symb.copy()
for i in range(n_vars):
    x_vals[i] = np.arange(0, 1, 1./N_points)
\end{lstlisting}
Because we are using Franke function \cite{Morten}:
\begin{lstlisting}
def FrankeFunction(x,y):
    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
    return term1 + term2 + term3 + term4
\end{lstlisting}
I am generating the response as:
\begin{lstlisting}
import reglib as rl
# library object instantiation
lib = rl.RegressionLibrary(x_symb, x_vals)
# setting up the grid
x, y = np.meshgrid(x_vals[0], x_vals[1])
# and getting output based on the Franke Function
z = lib.FrankeFunction(x, y) + 0.1 * np.random.randn(N_points, N_points)
\end{lstlisting}
and, for each polynomial (up to specified max value), I am instantiating the pipeline, which calculates OLS, Ridge and LASSO regressions with several different hyper parameters
\begin{lstlisting}
for poly_degree in range(1, max_poly_degree+1):
    print('\n')
    print('Starting analysis for polynomial of degree: ' + str(poly_degree))
    pipeline = MainPipeline(x_symb, x_vals, x, y, z, confidence, sigma, kfold, lambda_par, output_dir, prefix, poly_degree)
    pipeline.doRegression()
\end{lstlisting}

\subsubsection{Design Matrix}
To construct a proper design matrix, I first need to generate the polynomial for appropriate degree. I am doing this by using \textbf{sympy} and \textbf{itertools} library to account for all possible combinations of multiplications between our variables, $x_0$, $x_1$, and 1 (i.e.
$x_0*x_1*1,$ $x_0*x_0*x_1*1$, ...) as:
\begin{lstlisting}
variables = list(self.x_symb.copy())
variables.append(1)
terms = [sp.Mul(*i) for i in it.combinations_with_replacement(variables, poly_degree)]
\end{lstlisting}
After that, I create the matrix and feed it with values, using \textbf{lambdify} method from \textbf{sympy} library:
\begin{lstlisting}
points = len(x_vals[0]) * len(x_vals[1])
X1 = np.ones((points, len(terms)))
for k in range(len(terms)):
    f = sp.lambdify([self.x_symb[0], self.x_symb[1]], terms[k], "numpy")
    X1[:, k] = [f(i, j) for i in self.x_vals[1] for j in self.x_vals[0]]
\end{lstlisting}
The whole method,which returns design matrix for a given polynomial degree is then:
\begin{lstlisting}
def constructDesignMatrix(self, *args):
    # the degree of polynomial to be generated
    poly_degree = args[0]
    # getting inputs
    x_vals = self.x_vals
    # using itertools for generating all possible combinations
    # of multiplications between our variables and 1, i.e.:
    # x_0*x_1*1, x_0*x_0*x_1*1 etc. => will get polynomial
    # coefficients
    variables = list(self.x_symb.copy())
    variables.append(1)
    terms = [sp.Mul(*i) for i in it.combinations_with_replacement(variables, poly_degree)]
    # creating desing matrix
    points = len(x_vals[0]) * len(x_vals[1])
    # creating desing matrix composed of ones
    X1 = np.ones((points, len(terms)))
    # populating design matrix with values
    for k in range(len(terms)):
        f = sp.lambdify([self.x_symb[0], self.x_symb[1]], terms[k], "numpy")
        X1[:, k] = [f(i, j) for i in self.x_vals[1] for j in self.x_vals[0]]
    # returning constructed design matrix (for 2 approaches if needed)
    return X1
\end{lstlisting}
With this in hand, I proceed for regression analysis.

\subsubsection{Linear Regression}
The main method here is dolinearRegression, which returns predicted values, regression coefficients and confidence intervals for $\beta$ coefficients. It is based on the \textbf{Singular Value Decomposition} (code is taken from \cite{Morten}):
\begin{lstlisting}
def doSVD(self, *args):
    # getting matrix
    X = args[0]
    # Applying SVD
    A = np.transpose(X) @ X
    U, s, VT = np.linalg.svd(A)
    D = np.zeros((len(U), len(VT)))
    for i in range(0, len(VT)):
        D[i, i] = s[i]
    UT = np.transpose(U)
    V = np.transpose(VT)
    invD = np.linalg.inv(D)
    invA = np.matmul(V, np.matmul(invD, UT))

    return invA
\end{lstlisting}
and takes the generated design matrix and the responses (generated by Franke function) as its inputs:
\begin{lstlisting}
def doLinearRegression(self, *args):
    # getting design matrix
    X = args[0]
    # getting z values and making them 1d
    z = np.ravel(args[1])
    # calculating variance of data

    # and then make the prediction
    invA = self.doSVD(X)
    beta = invA.dot(X.T).dot(z)
    ztilde = X @ beta
    # calculating beta confidence
    confidence = args[2]  # 1.96
    sigma = args[3]#np.var(z)  # args[3] #1
    SE = sigma * np.sqrt(np.diag(invA)) * confidence
    beta_min = beta - SE
    beta_max = beta + SE

    return ztilde, beta, beta_min, beta_max
\end{lstlisting}
For a given polynomial degree I get a set of $\beta$'s, which I plot and save inside "Output" folder.

\subsubsection{Ridge regression}

The main method here is doRidgeRegression. It works similarly to the doLinearRegression method, but it takes one more input - hyper parameter:
\begin{lstlisting}
lambda_par = args[2]
\end{lstlisting}
and constructs the identity matrix as:
\begin{lstlisting}
XTX = X.T.dot(X)
I = np.identity(len(XTX), dtype=float)
\end{lstlisting}
which allows calculation of $\beta$'s as:
\begin{lstlisting}
invA = np.linalg.inv(XTX + lambda_par * I)
beta = invA.dot(X.T).dot(z)
\end{lstlisting}
Thus the entire method is
\begin{lstlisting}
def doRidgeRegression(self, *args):
    # getting design matrix
    X = args[0]
    # getting z values
    z = np.ravel(args[1])
    # hyper parameter
    lambda_par = args[2]
    # constructing the identity matrix
    XTX = X.T.dot(X)
    I = np.identity(len(XTX), dtype=float)
    # calculating parameters
    invA = np.linalg.inv(XTX + lambda_par * I)
    beta = invA.dot(X.T).dot(z)
    # and making predictions
    ztilde = X @ beta

    # calculating beta confidence
    confidence = args[3]  # 1.96
    # calculating variance
    sigma = args[4]#np.var(z)  # args[4] #1
    SE = sigma * np.sqrt(np.diag(invA)) * confidence
    beta_min = beta - SE
    beta_max = beta + SE

    return ztilde, beta, beta_min, beta_max
\end{lstlisting}

\subsubsection{Lasso}

For Lasso Regression I have used only the Scikit learn functionalities:
\begin{lstlisting}
lasso_reg = Lasso(alpha=self.lambda_par).fit(X_poly, z_rav)
ztilde_sk = lasso_reg.predict(X_poly).reshape(zshape)
\end{lstlisting}

\subsubsection{Cross Validation}
Because the original task for Linear Regression analysis was split into several parts, I wrote code for k-Fold cross validation as a separate method. The idea for a division is as follows:
\begin{itemize}
    \item Instead of two groups, we must return k-folds or k groups of data.
    \item If the dataset does not cleanly divide by the number of folds, there may be some remainder rows and they will not be used in the split. This means that we need first to account for this division. I have written method which accounts for that, and also shuffling the data set randomly:
\end{itemize}
\begin{lstlisting}
def shuffleDataSimultaneously(self, a, b):
    assert len(a) == len(b)
    p = np.random.permutation(len(a))
    return a[p], b[p]
\end{lstlisting}
and then divides it into equal kfolds:
\begin{lstlisting}
def splitDataset(self, *args):
    # getting inputs
    X = args[0]
    z = args[1]
    kfold = args[2]
    iterator = args[3]
    # If the dataset does not cleanly divide by the number of folds,
    # there may be some remainder rows and they will not be used in the split.
    length = len(X) % kfold
    if length == 0:
        condition = True
    else:
        condition = False
    while condition is False:
        # removing the element <= they were shuffled randomly,
        # so it doesn't matter which one to remove
        X = np.delete(X, -1, axis = 0)
        z = np.delete(z, -1, axis = 0)
        # checking whether it is divided cleanly
        length = len(X) % kfold
        if length == 0:
            condition = True
    # 2. Split the dataset into k groups:
    X_split = np.array_split(X, kfold, axis=0)
    z_split = np.array_split(z, kfold, axis=0)
    # train data set - making a copy of the shuffled and splitted arrays
    X_train = X_split.copy()
    z_train = z_split.copy()
    # test data set - each time new element
    X_test = X_split[iterator]
    z_test = z_split[iterator]
    # deleting current element
    X_train = np.delete(X_train, iterator, 0)
    z_train = np.delete(z_train, iterator, 0)
    # and adjusting arrays dimensions (e.g. X: [4, 500, 21] => [2000, 21])
    X_train = np.concatenate(X_train, axis=0)
    z_train = z_train.ravel()

    return X_train, X_test, z_train, z_test
\end{lstlisting}
The Cross validation itself is done via
\begin{lstlisting}
def doCrossVal(self, *args):
    # getting design matrix
    X = args[0]
    # getting z values and making them 1d
    z = np.ravel(args[1])
    kfold = args[2]
    MSEtest_lintot = []
    MSEtrain_lintot = []
    z_tested = []
    z_trained = []
    z_t = []
    # bias
    bias = []
    # shuffling dataset randomly
    # 1. Shuffling datasets randomly:
    X, z = self.shuffleDataSimultaneously(X, z)
    # splitting data sets into the kfold and iterate over each of them
    for i in range(kfold):
        # Splitting and shuffling data randomly
        X_train, X_test, z_train, z_test = self.splitDataset(X, z, kfold, i)
        z_t.append(z_test)
        # Train The Pipeline
        invA = self.doSVD(X_train)
        beta_train = invA.dot(X_train.T).dot(z_train)
        # Testing the pipeline
        z_trained.append(X_train @ beta_train)
        z_tested.append(X_test @ beta_train)
        # Calculating MSE for each iteration
        MSEtest_lintot.append(self.getMSE(z_test, z_tested[i]))
        MSEtrain_lintot.append(self.getMSE(z_train, z_trained[i]))
    # linear MSE
    MSEtest_lin = np.mean(MSEtest_lintot)
    MSEtrain_lin = np.mean(MSEtrain_lintot)
    # bias-variance trade off
    z_tested_mean = np.mean(z_tested, axis=1, keepdims=True)
    for i in range(kfold):
        bias.append((z_t[i] - z_tested_mean)**2)
    bias_mean = np.mean( bias )
    variance_mean = np.mean( np.var(z_tested, axis=1, keepdims=True) )

    return MSEtest_lin, MSEtrain_lin, bias_mean, variance_mean
\end{lstlisting}
which uses singular value decomposition to calculate $\beta$. The method returns the Mean Squared Error for test and training data sets, as well as bias and variance for a given polynomial. The code to calculate MSEs is taken from \cite{Morten}:
\begin{lstlisting}
def getMSE(self, z_data, z_model):
    n = np.size(z_model)
    return np.sum((z_data - z_model) ** 2) / n
\end{lstlisting}
Cross validation for Ridge regression is done in a similar manner, which includes hyper parameter $\lambda$. This method is called "doCrossValRidge".

For Lasso regression I am using Scikit Learn methods and, because, I am also comparing my results with the ones from Scikit Learn (only for Linear and Ridge), I wrote a single method, which handles every regression type for CV. It is based on Scikit Learn functionality. 

The important parameter is a string variable which decides what regression algorithm to use:
\begin{lstlisting}
reg_type = args[5]
if reg_type == 'linear':
    model = LinearRegression(fit_intercept = False)
elif reg_type == 'ridge':
    model = Ridge(alpha = lambda_par, fit_intercept = False)
elif reg_type == 'lasso':
    model = Lasso(alpha = lambda_par)
else:
    print("Houston, we've got a problem!")
\end{lstlisting}
With this, I go directly to Scikit functionalities for splitting data set into k parts. The entire method is then:
\begin{lstlisting}
def doCrossValScikit(self, *args):
        # getting inputs
        X = args[0]
        z = args[1]
        kfold = args[2]
        poly_degree = args[3]
        lambda_par = args[4]
        # understanding the regression type to use
        reg_type = args[5]
        if reg_type == 'linear':
            model = LinearRegression(fit_intercept = False)
        elif reg_type == 'ridge':
            model = Ridge(alpha = lambda_par, fit_intercept = False)
        elif reg_type == 'lasso':
            model = Lasso(alpha = lambda_par)
        else:
            print("Houston, we've got a problem!")

        MSEtest = []
        MSEtrain = []
        # bias
        bias = []
        z_t = []
        z_tested = []
        # If the dataset does not cleanly divide by the number of folds,
        # there may be some remainder rows and they will not be used in the split.
        length = len(X) % kfold
        if length == 0:
            condition = True
        else:
            condition = False
        while condition is False:
            # removing the element <= they were shuffled randomly,
            # so it doesn't matter which one to remove
            X = np.delete(X, -1, axis = 0)
            z = np.delete(z, -1, axis = 0)
            # checking whether it is divided cleanly
            length = len(X) % kfold
            if length == 0:
                condition = True
        # making splits - shuffling it
        cv = KFold(n_splits = kfold, shuffle = True, random_state = 1)
        # enumerate splits - splitting the data set to train and test splits
        for train, test in cv.split(X):
            X_train, X_test = X[train], X[test]
            z_train, z_test = z[train], z[test]
            z_t.append(z_test)
            # making the prediction - comparing outputs for current and "future" datasets
            z_tilde = model.fit(X_train, z_train).predict(X_train).ravel() # z_trained
            z_pred = model.fit(X_train, z_train).predict(X_test).ravel() # z_tested
            z_tested.append(z_pred)

            MSEtest.append(mean_squared_error(z_test, z_pred))
            MSEtrain.append(mean_squared_error(z_train, z_tilde))

        # getting the mean values for errors (to plot them later)
        MSEtest_mean = np.mean(MSEtest)
        MSEtrain_mean = np.mean(MSEtrain)
        # bias-variance trade off
        z_tested_mean = np.mean(z_tested, axis=1, keepdims=True)
        for i in range(kfold):
            bias.append((z_t[i] - z_tested_mean)**2)
        bias_mean = np.mean( bias )
        variance_mean = np.mean( np.var(z_tested, axis=1, keepdims=True) )

        # returning MSE, bias and variance for  a given polynomial degree
        return MSEtest_mean, MSEtrain_mean, bias_mean, variance_mean
\end{lstlisting}

The method which are located in the regression.py is mostly for plotting, saving and printing various values.

\subsection{Real Data - Bug fixes}

The real data uses exactly the same method described above. The only difference is that instead of generating function using Franke function, I retrieve it from the tif file. 

The problem with the real data here is that the data set is pretty big. For the first run, I didn't account for it, and that is why it took me more than 7 hours to finish calculating things for 5 consecutive polynomials. To be precise, the console output was:
\begin{lstlisting}
-- Program finished at 26326.446545124054 sec --
\end{lstlisting}
But, as we will see later, there is no actual need to run the entire data set. In fact, some of our fellow students mentioned that $10\%$ is more than enough to run as the amount of points to train and test the model. I was running even lesser amount. All in all, I changed script a little bit in order to run only portion of the data set
\begin{lstlisting}
sys.stdout.write("Please, specify the percentage of the data to use (default = 0.1): ")
cut_dat = input()
terrain.sort()
if cut_dat == '':
    dataset = terrain[int(len(terrain) * 0.9):]#terrain[int(len(terrain) * .05) : int(len(terrain) * .95)]
else:
    dataset = terrain[int(len(terrain) * (1.-float(cut_dat))):]
print(np.shape(dataset))
\end{lstlisting}
This part is located inside \textbf{regression.py} after:
\begin{lstlisting}
if __name__ == '__main__':
\end{lstlisting}
statement.

After running the script you will be specifically asked what part of the data set you want to use (from 0 to 1, with 1 being the largest value). There is, of course, a default value of $10\%$.

Basically, after running the script with the command above, you will be asked several questions about the data set you want to use and other small details such as the minimal value of $\lambda$ to use, the max degree of polynomial, the number of points to generate

The script \textbf{regression.py} is quite long, but it mostly contains calling for the methods for calculation and ploting fitted surfaces, MSEs, $\beta$, confidence intervals etc. It also saves values, such as regression coefficients and confidence intervals inside \textbf{txt} files to be able to access them later on.





