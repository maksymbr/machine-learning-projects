{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/maksymb/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/maksymb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3155 - acc: 0.9100 - val_loss: 0.1646 - val_acc: 0.9522\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.1502 - acc: 0.9565 - val_loss: 0.1285 - val_acc: 0.9619\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.1169 - acc: 0.9664 - val_loss: 0.1032 - val_acc: 0.9661\n",
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# The full CNN code!\n",
    "####################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = mnist.train_images() \n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('cnn.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # number of hidden layers\n",
    "        self.nHidden = args[0]\n",
    "\n",
    "    # creating siamese network\n",
    "    def CreateSiamese(self, *args):\n",
    "        # the shape of the inputs\n",
    "        input_shape = args[0]\n",
    "        # creating a model variable\n",
    "        model = keras.models.Sequential()\n",
    "        # adding convolutional layers\n",
    "        for layer in range (1, layers):\n",
    "            # configuring first layer\n",
    "            if layer == 0:\n",
    "                configuration = Dense(nHidden, activation=NNArch[layer]['AF'], \\\n",
    "                                      kernel_initializer='random_normal', input_shape=input_shape)\n",
    "            # configuring last layer\n",
    "            elif layer == nLayers-1:\n",
    "                configuration = Dense(nOutput, activation=NNArch[layer]['AF'], \\\n",
    "                                     kernel_initializer='random_normal')\n",
    "            # configuring layers in between\n",
    "            else:\n",
    "                configuration = Dense(nHidden, activation=NNArch[layer]['AF'], \\\n",
    "                                     kernel_initializer='random_normal')\n",
    "            model.add(configuration)\n",
    "        \n",
    "        # returning configured model\n",
    "        return model\n",
    "\n",
    "                \n",
    "                # Feed Forward Neural Network\n",
    "def CallKerasModel(NNArch, nLayers):\n",
    "    classifier = Sequential()\n",
    "    print(\"nHidden\", nHidden)\n",
    "\n",
    "    for layer in range(1, nLayers):\n",
    "        if layer == 0:\n",
    "            classifier.add(Dense(nHidden, activation=NNArch[layer]['AF'], \\\n",
    "                                 kernel_initializer='random_normal', input_dim=nFeatures))\n",
    "        elif layer == nLayers-1:\n",
    "            classifier.add(Dense(nOutput, activation=NNArch[layer]['AF'], \\\n",
    "                                 kernel_initializer='random_normal'))\n",
    "        else:\n",
    "            classifier.add(Dense(nHidden, activation=NNArch[layer]['AF'], \\\n",
    "                                 kernel_initializer='random_normal'))\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_4/cond/Merge:0\", shape=(?, 128), dtype=float32)\n",
      "Train on 108400 samples, validate on 17820 samples\n",
      "Epoch 1/20\n",
      "108400/108400 [==============================] - 3s 29us/step - loss: 0.0944 - accuracy: 0.8899 - val_loss: 0.0406 - val_accuracy: 0.9594\n",
      "Epoch 2/20\n",
      "108400/108400 [==============================] - 3s 25us/step - loss: 0.0384 - accuracy: 0.9628 - val_loss: 0.0300 - val_accuracy: 0.9671\n",
      "Epoch 3/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0272 - accuracy: 0.9723 - val_loss: 0.0271 - val_accuracy: 0.9691\n",
      "Epoch 4/20\n",
      "108400/108400 [==============================] - 3s 25us/step - loss: 0.0216 - accuracy: 0.9781 - val_loss: 0.0236 - val_accuracy: 0.9728\n",
      "Epoch 5/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0187 - accuracy: 0.9802 - val_loss: 0.0240 - val_accuracy: 0.9718\n",
      "Epoch 6/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0164 - accuracy: 0.9826 - val_loss: 0.0222 - val_accuracy: 0.9743\n",
      "Epoch 7/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0147 - accuracy: 0.9842 - val_loss: 0.0216 - val_accuracy: 0.9733\n",
      "Epoch 8/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0135 - accuracy: 0.9857 - val_loss: 0.0231 - val_accuracy: 0.9735\n",
      "Epoch 9/20\n",
      "108400/108400 [==============================] - 3s 27us/step - loss: 0.0129 - accuracy: 0.9862 - val_loss: 0.0221 - val_accuracy: 0.9730\n",
      "Epoch 10/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0120 - accuracy: 0.9872 - val_loss: 0.0210 - val_accuracy: 0.9751\n",
      "Epoch 11/20\n",
      "108400/108400 [==============================] - 3s 26us/step - loss: 0.0113 - accuracy: 0.9880 - val_loss: 0.0215 - val_accuracy: 0.9741\n",
      "Epoch 12/20\n",
      "108400/108400 [==============================] - 3s 27us/step - loss: 0.0109 - accuracy: 0.9883 - val_loss: 0.0220 - val_accuracy: 0.9729\n",
      "Epoch 13/20\n",
      "108400/108400 [==============================] - 3s 27us/step - loss: 0.0102 - accuracy: 0.9888 - val_loss: 0.0212 - val_accuracy: 0.9742\n",
      "Epoch 14/20\n",
      "108400/108400 [==============================] - 3s 27us/step - loss: 0.0099 - accuracy: 0.9896 - val_loss: 0.0214 - val_accuracy: 0.9739\n",
      "Epoch 15/20\n",
      "108400/108400 [==============================] - 3s 27us/step - loss: 0.0095 - accuracy: 0.9898 - val_loss: 0.0232 - val_accuracy: 0.9731\n",
      "Epoch 16/20\n",
      "108400/108400 [==============================] - 3s 28us/step - loss: 0.0093 - accuracy: 0.9900 - val_loss: 0.0221 - val_accuracy: 0.9741\n",
      "Epoch 17/20\n",
      "108400/108400 [==============================] - 3s 31us/step - loss: 0.0089 - accuracy: 0.9907 - val_loss: 0.0229 - val_accuracy: 0.9730\n",
      "Epoch 18/20\n",
      "108400/108400 [==============================] - 3s 29us/step - loss: 0.0089 - accuracy: 0.9904 - val_loss: 0.0224 - val_accuracy: 0.9738\n",
      "Epoch 19/20\n",
      "108400/108400 [==============================] - 4s 32us/step - loss: 0.0085 - accuracy: 0.9911 - val_loss: 0.0224 - val_accuracy: 0.9738\n",
      "Epoch 20/20\n",
      "108400/108400 [==============================] - 3s 29us/step - loss: 0.0083 - accuracy: 0.9911 - val_loss: 0.0235 - val_accuracy: 0.9722\n",
      "* Accuracy on training set: 99.60%\n",
      "* Accuracy on test set: 97.22%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n",
    "    for d in range(num_classes):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, num_classes)\n",
    "            dn = (d + inc) % num_classes\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# create training+test positive and negative pairs\n",
    "digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n",
    "tr_pairs, tr_y = create_pairs(x_train, digit_indices)\n",
    "\n",
    "digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\n",
    "te_pairs, te_y = create_pairs(x_test, digit_indices)\n",
    "\n",
    "# network definition\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n",
    "\n",
    "# compute final accuracy on training and test sets\n",
    "y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "tr_acc = compute_accuracy(tr_y, y_pred)\n",
    "y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(te_y, y_pred)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"flatten_7/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"dense_14/Relu:0\", shape=(?, 128), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x62b1db4e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "create_base_network((100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running siamese network\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "#from keras import backend as K\n",
    "\n",
    "import time\n",
    "\n",
    "'''\n",
    "Class which has all necessary functions\n",
    "'''\n",
    "class Functions:\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        # passing the random seed\n",
    "        seed = args[0]\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Initialising Weights\n",
    "    def GetWeights(self, *args):\n",
    "        # the value from parameter file\n",
    "        name = args[0]\n",
    "        # random seed\n",
    "        #seed = args[1]\n",
    "        if name == 'norm':\n",
    "            # random normal\n",
    "            return keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=seed)\n",
    "        elif name == 'unif':\n",
    "            # random uniform\n",
    "            return keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=seed)\n",
    "        elif name == 'xnorm':\n",
    "            # Xavier normal\n",
    "            return keras.initializers.glorot_normal(seed=seed)\n",
    "        elif name == 'xunif':\n",
    "            # Xavier uniform\n",
    "            return keras.initializers.glorot_uniform(seed=seed)\n",
    "        elif name == 'hnorm':\n",
    "            # He normal\n",
    "            return keras.initializers.he_normal(seed=seed)\n",
    "        elif name == 'hunif': \n",
    "            # He uniform\n",
    "            return keras.initializers.he_uniform(seed=seed)\n",
    "        else:\n",
    "            print(\"Check your weights!\")\n",
    "            sys.exit()\n",
    "            \n",
    "    # Activation Functions\n",
    "    def GetActivation(self, *args):\n",
    "        # name of activation functions\n",
    "        name = args[0]\n",
    "        #\n",
    "        x = args[1]\n",
    "        if name == 'linear':\n",
    "            # Linear\n",
    "            return keras.activations.linear(x)\n",
    "        elif name == 'exp':\n",
    "            # Exponential\n",
    "            return keras.activations.exponential(x)\n",
    "        elif name == 'tanh':\n",
    "            # Tanh\n",
    "            return keras.activations.tanh(x)\n",
    "        elif name == 'sigmoid':\n",
    "            # Sigmoid\n",
    "            return keras.activations.sigmoid(x)\n",
    "        elif name == 'hsigmoid':\n",
    "            # Hard Sigmoid\n",
    "            return keras.activations.hard_sigmoid(x)\n",
    "        elif name == 'softmax':\n",
    "            # Softmax\n",
    "            return keras.activations.softmax(x, axis=-1)\n",
    "        elif name == 'softplus':\n",
    "            # Softplus\n",
    "            return keras.activations.softplus(x)\n",
    "        elif name == 'softsign':\n",
    "            # Softsign\n",
    "            return keras.activations.softsign(x)\n",
    "        elif name == 'relu':\n",
    "            # ReLU\n",
    "            return keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)\n",
    "        elif name == 'elu':\n",
    "            # eLU\n",
    "            return keras.activations.elu(x, alpha=1.0)\n",
    "        elif name == 'selu':\n",
    "            # SeLU\n",
    "            return keras.activations.selu(x)\n",
    "        else:\n",
    "            print('Check activation function!')\n",
    "            sys.exit()\n",
    "    \n",
    "    # Regularization (L1, L2 and combined to avoid overfitting)\n",
    "    def GetRegularizer(self, *args):\n",
    "        name = args[0]\n",
    "        \n",
    "        if name == 'l1':\n",
    "            return keras.regularizers.l1(0.)\n",
    "        elif name == 'l2':\n",
    "            return keras.regularizers.l2(0.)\n",
    "        elif name == 'l1l2':\n",
    "            return keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "        else:\n",
    "            print('Check Regularization')\n",
    "            sys.exit()\n",
    "            \n",
    "    # Optimization algorithms (to compile and run the model), \n",
    "    # i.e. various gradients methods\n",
    "    def GetGradient(self, *args):\n",
    "        # the name of the method\n",
    "        name = args[0]\n",
    "        # learning rate\n",
    "        alpha = args[1]\n",
    "        if name == 'sgd':\n",
    "            # Stochastic Gradient Descent - includes momentum and support for Nesterov momentum\n",
    "            return keras.optimizers.SGD(learning_rate=alpha, momentum=0.0, nesterov=False)\n",
    "            # Nesterov\n",
    "        elif name == 'nesterov':\n",
    "            return keras.optimizers.SGD(learning_rate=alpha, momentum=0.0, nesterov=True)\n",
    "        elif name == 'rmsprop':\n",
    "            # RMSProp\n",
    "            return keras.optimizers.RMSprop(learning_rate=alpha, rho=0.9)\n",
    "        elif name == 'adagrad':\n",
    "            # Adagrad\n",
    "            return keras.optimizers.Adagrad(learning_rate=alpha)\n",
    "        elif name == 'adadelta':\n",
    "            # Adadelta\n",
    "            return keras.optimizers.Adadelta(learning_rate=alpha, rho=0.95)\n",
    "        elif name == 'adam':\n",
    "            # Adam\n",
    "            return keras.optimizers.Adam(learning_rate=alpha, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        elif name == 'adamax':\n",
    "            # Adamax\n",
    "            return keras.optimizers.Adamax(learning_rate=alpha, beta_1=0.9, beta_2=0.999)\n",
    "        elif name == 'nadam':\n",
    "            # Nadam\n",
    "            return keras.optimizers.Nadam(learning_rate=alpha, beta_1=0.9, beta_2=0.999)\n",
    "        else:\n",
    "            print('Check optimization Methods!')\n",
    "            sys.exit()\n",
    "            \n",
    "    # Getting Loss function\n",
    "    def GetLoss(self, *args):\n",
    "        name   = args[0]\n",
    "        y_true = args[1]\n",
    "        y_pred = args[2]\n",
    "        # Getting the correct loss function\n",
    "        if name == 'mse':\n",
    "            # Mean Squared Error (MSE)\n",
    "            return keras.losses.mean_squared_error(y_true, y_pred)\n",
    "        elif name == 'mae':\n",
    "            # Mean Absolute Error (MAE)\n",
    "            return keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "        elif name == 'mape':\n",
    "            # Mean Absolute Percentage Error (MAPE)\n",
    "            return keras.losses.mean_absolute_percentage_error(y_true, y_pred)\n",
    "        elif name == 'msle': \n",
    "            # Mean Squared Logarithmic Error (MSLE)\n",
    "            return keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n",
    "        elif name == 'hinge':\n",
    "            # Hinge\n",
    "            return keras.losses.hinge(y_true, y_pred)\n",
    "        elif name == 'shinge':\n",
    "            # Squared Hinge\n",
    "            return keras.losses.squared_hinge(y_true, y_pred)\n",
    "        elif name == 'chinge':\n",
    "            # Categorical Hinge\n",
    "            return keras.losses.categorical_hinge(y_true, y_pred)\n",
    "        elif name == 'logcosh':\n",
    "            # LogCosh\n",
    "            return keras.losses.logcosh(y_true, y_pred)\n",
    "        elif name == 'huber':\n",
    "            # Huber Loss\n",
    "            return keras.losses.huber_loss(y_true, y_pred, delta=1.0)\n",
    "        elif name == 'categorical':\n",
    "            # Categorical Cross Entropy\n",
    "            return keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)\n",
    "        elif name == 'sparse':\n",
    "            # Sparse Categorical Cross Entropy\n",
    "            return keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
    "        elif name == 'binary':\n",
    "            # Binary Cross Entropy\n",
    "            return keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)\n",
    "        elif name == 'kullback':\n",
    "            # Kullback Leibler Divergence\n",
    "            return keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "        elif name == 'poisson':\n",
    "            # Poisson\n",
    "            return keras.losses.poisson(y_true, y_pred)\n",
    "        elif name == 'proximity':\n",
    "            # Cosine Proximity\n",
    "            return keras.losses.cosine_proximity(y_true, y_pred, axis=-1)\n",
    "        elif name == 'contrasive':\n",
    "            '''Contrastive loss from Hadsell-et-al.'06\n",
    "            http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "            '''\n",
    "            margin = 1\n",
    "            square_pred = keras.backend.square(y_pred)\n",
    "            margin_square = keras.backend.square(keras.backend.maximum(margin - y_pred, 0))\n",
    "            return keras.backend.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "        else:\n",
    "            print(\"Check Loss function!\")\n",
    "            sys.exit()\n",
    "\n",
    "'''\n",
    "Neural Network Class\n",
    "'''\n",
    "class NeuralNetwork:\n",
    "    # Constructor - Getting Network Configuration\n",
    "    def __init__(self, *args):\n",
    "        # Network Type\n",
    "        self.NNType = args[0]\n",
    "        # Network Architecture\n",
    "        self.NNArch = args[1]\n",
    "        # Total Number of layers\n",
    "        self.nLayers = args[2]\n",
    "        # Neurons in Input Layer\n",
    "        self.nInputNeurons = args[3]\n",
    "        # Neurons in Hidden Layer\n",
    "        self.nHiddenNeurons = args[4]\n",
    "        # Neurons in Output Layer\n",
    "        self.nOutputNeurons = args[5]\n",
    "        # random seed\n",
    "        seed = args[6]\n",
    "        '''\n",
    "        # Initializing object variable\n",
    "        self.funcs = Functions(seed)\n",
    "        # number of iterations for optimization algorithm\n",
    "        self.epochs = args[6]\n",
    "        # learning rate\n",
    "        self.alpha = args[7]\n",
    "        # regularisation (hyper) parameter\n",
    "        self.lambd = args[8]\n",
    "        # Data\n",
    "        self.nInput = args[9]\n",
    "        #self.nFeatures = \n",
    "        seed = args[10]\n",
    "        # Batch Size\n",
    "        self.BatchSize = args[11]\n",
    "        # random seed, to make the same random number each time\n",
    "        np.random.seed(seed)\n",
    "        # optimization algorithm\n",
    "        self.optimization = args[12]\n",
    "        '''\n",
    "\n",
    "        \n",
    "        # Printing current network configuration\n",
    "        \"\"\"\n",
    "        print((u'''\n",
    "        =========================================== \n",
    "            Start {} Neural Network \n",
    "        =========================================== \n",
    "        No. of hidden layers:        {} \n",
    "        No. of input data:           {}\n",
    "        No. of input neurons:        {} \n",
    "        No. of hidden neurons:       {} \n",
    "        No. of output neurons:       {} \n",
    "        Activ. Func in Hidden Layer: {} \n",
    "        Activ. Func in Output Layer: {} \n",
    "        No. of epochs to see:        {}\n",
    "        Optimization Algorithm:      {}\n",
    "        Learning Rate, \\u03B1:            {} \n",
    "        Regularization param, \\u03BB:     {} \n",
    "                      '''.format(self.NNType,\n",
    "                                 self.nLayers-2,\n",
    "                                 self.nInput,\n",
    "                                 self.nInputNeurons, \n",
    "                                 self.nHiddenNeurons, \n",
    "                                 self.nOutputNeurons,\n",
    "                                 self.NNArch[1]['AF'],\n",
    "                                 self.NNArch[self.nLayers-1]['AF'],\n",
    "                                 self.epochs,\n",
    "                                 self.optimization,\n",
    "                                 self.alpha,\n",
    "                                 self.lambd)))\n",
    "        \"\"\"\n",
    "    \n",
    "    # Creating the model\n",
    "    def BuildModel(self, *args):\n",
    "        name = args[0]\n",
    "        # shape of the inputs\n",
    "        inputShape = args[1]\n",
    "        # shape \n",
    "        \n",
    "        # customizing our model\n",
    "        inputs = Input(shape=inputShape)\n",
    "        # adding (connecting) layers\n",
    "        for layer in range(layers):\n",
    "            # first layere\n",
    "            if layer == 0:\n",
    "                # getting activation function for the first layer\n",
    "                activation = self.funcs.GetActivation(self.NNArch[layer]['AF'])\n",
    "                hidden = Dense(2, activation = activation)(inputs)\n",
    "            # last layer\n",
    "            elif layer == layers:\n",
    "                # getting activation function for the last layer\n",
    "                activation = self.NNArch[layer]['AF']\n",
    "                outputs = Dense(2, activation = activation)(hidden)\n",
    "            # intermediate layers\n",
    "            else:\n",
    "                # getting activation function for the hidden layers\n",
    "                activation = self.NNArch[layer]['AF']\n",
    "                hidden = Dense(2, activation = activation)(hidden)\n",
    "        # building the model\n",
    "        model = Model(inputs = inputs, outputs = outputs)\n",
    "        # returning the constructed model\n",
    "        return model\n",
    "    \n",
    "'''\n",
    "The main class of the program\n",
    "'''\n",
    "class Main:\n",
    "    def __init__(self, *args):\n",
    "        # getting the type of the program\n",
    "        self.type = args[0]\n",
    "        seed = args[1]\n",
    "        # instantiating the Neural Network class variable\n",
    "        self.network = NeuralNetwork(seed)\n",
    "    \n",
    "    def RunPipeline(self, *args):\n",
    "        \n",
    "        if self.type == 'siamese':\n",
    "            print('Running siamese network')\n",
    "        elif self.type == 'xgboost':\n",
    "            print('Running XGBoost')\n",
    "        elif self.type == 'rf':\n",
    "            print('Running Random Forest')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    type = 'siamese'\n",
    "    pipe = Main(type)\n",
    "    pipe.RunPipeline()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
