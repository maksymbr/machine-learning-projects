#======================================================================================================================#
# Parameter File
#======================================================================================================================#
# Type of task to address
type:     'classification' # ['rnet', 'cnet', 'xg', 'rf'] <= Neural Network, XGBoost, Random Forest
# Path to data
dataPath:       ['data/ASV_table_mod.csv','data/Metadata_table.tsv'] #'data/ASV_table_mod.csv'
# Output Path - where to save all the files (phg's etc.)
outputPath:     'output/'
# Specify the seed
RandomSeed:     2
#======================================================================================================================#
# Neural Network Configuration (ignore it, if you are using xgboost or random forest)
#======================================================================================================================#
# The entropy to use
Loss:           'binary' #'binary'
# Weights
Weights:        'xnorm' # ['norm', 'xnorm','hnorm', 'unif', 'xunif', 'hunif']
# Choose number of layers (can be any number)
NHiddenLayers:  1
# Activation functions for hidden and output layers
# (['sigmoid', 'tanh', 'relu', 'softmax'])
HiddenFunc:     'relu' #'sigmoid'
OutputFunc:     'softmax'
# Size of the test sample
TestSize:       0.5
# Number of Neurons for hidden and output layers
NHiddenNeurons: 15 # 21 # 4  # 30
NOutputNeurons: 2 # classification = 2, Regression = 1
# Epochs to train the algorithm
epochs:         300
# Optimization Algorithm: choose it wisely :)
#['MBGD', 'Adagrad' 'GD'] <= for minibatch, if you choose 1 you will get just stochstic GD
# if you choose simply GD, then it willl ignore batchSize parameter and will use the whole data set
Optimization:   'adagrad' # please use Adagrad for linear regression (as it may crush overwise
# Batch size for Gradient Descent => if 0, will use simple gradient descent
BatchSize:      1 #16 # 1 #10000 <= increase batch size and it will be good
# Learning rate
alpha:          0.01 # 0.01 # 0.01 #0.0001 #np.logspace(-5, 1, 7)
# Regularisation
lambda:         0.0001