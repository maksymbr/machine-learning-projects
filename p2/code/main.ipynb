{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8617ec549dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mx_rav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_rav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mnInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataProc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         '''\n\u001b[1;32m    596\u001b[0m         \u001b[0mPolynomial\u001b[0m \u001b[0mRegression\u001b[0m \u001b[0mon\u001b[0m \u001b[0mFranke\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 13 11:21:05 2019\n",
    "\n",
    "@author: maksymb\n",
    "\"\"\"\n",
    "\n",
    "# importing libraries\n",
    "import os\n",
    "import sys\n",
    "import math as mt\n",
    "import numpy as np\n",
    "# for polynomial manipulation\n",
    "import sympy as sp\n",
    "# from sympy import *\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sbn\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from IPython.display import display, Latex, Markdown\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# to read parameter file\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Scikitlearn imports to check results\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from numpy import argmax\n",
    "\n",
    "# We'll need some metrics to evaluate our models\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "\n",
    "import keras\n",
    "# stochastic gradient descent\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# importing manually created libraries\n",
    "import funclib\n",
    "import neural\n",
    "import regression\n",
    "import data_processing\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def DoGriSearchRegression(logisticData, i, j, alpha, lmbd):\n",
    "    \n",
    "    '''\n",
    "    1. Big Plot of Franke Function and Corresponding Gradient analysis\n",
    "    2. Plots of MSE for different parameters (lambda, alpha, bumber of neurons?)\n",
    "    3. Plots of R2 for different values of the same parameters\n",
    "    4. Plots of cost function?\n",
    "    '''\n",
    "    \n",
    "    return print(i, j, k, l)\n",
    "\n",
    "def DoGriSearchClassification(logisticData, i, j, alpha, ambd):\n",
    "    # passing data values\n",
    "    NNType, NNArch, nLayers, nFeatures, \\\n",
    "        nHidden, nOutput, epochs, \\\n",
    "        X, X_train, X_test, Y_train, Y_test, Y_train_onehot, Y_test_onehot,\\\n",
    "        m, nInput, seed, onehotencoder,\\\n",
    "        BatchSize, Optimization = logisticData\n",
    "\n",
    "\n",
    "    pipeline = regression.RegressionPipeline()\n",
    "    # Getting cost function averaged over all epochs\n",
    "    costs, theta = pipeline.DoLogisticRegression(X_train, \\\n",
    "                                                Y_train, epochs, lmbd, alpha)\n",
    "    # getting the resulting values, to plot andto fit later on\n",
    "    #myResult = {alpha: [costs, theta], lambd: [costs, theta]}\n",
    "    myResult = {'costs': costs, 'theta': theta, 'lmbd': lmbd, 'alpha': alpha}\n",
    "    # fitting parameters to the data and evaluate all tests etc.\n",
    "    Y_pred = pipeline.PredictLogisticRegression(X_test, theta)\n",
    "    #print('ihgvfghcjghcghhhhhhhhhhhhhhhhhhhhhhhfgchcjhvgmvhjvhvhvhvh')\n",
    "    # Evaluating scores\n",
    "    #funclib.ErrorFuncs.own_classification_report(Y_train, Y_pred)\n",
    "    #Accuracy, Precision, Recall, F1 = funclib.ErrorFuncs.CallF1(Y_train, Y_pred)\n",
    "    # adding result to the dictionary\n",
    "    #myResult[alpha] = [Accuracy, Precision, Recall, F1]\n",
    "    #myResult[alpha][Accuracy, Precision, Recall, F1]\n",
    "    ytrue = Y_train\n",
    "    pred = Y_pred\n",
    "    threshold=0.5\n",
    "    return_f1=False\n",
    "    return_ac=False\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    pred=np.where(pred>threshold,1,0)\n",
    "    for i in range(len(ytrue)):\n",
    "        if (pred[i]==1 and ytrue[i]==1):\n",
    "            tp +=1\n",
    "        elif (pred[i]==1 and ytrue[i]==0):\n",
    "            fp +=1\n",
    "        elif (pred[i]==0 and ytrue[i]==0):\n",
    "            tn +=1\n",
    "        elif (pred[i]==0 and ytrue[i]==1):\n",
    "            fn +=1\n",
    "    pcp=np.sum(np.where(pred==1,1,0))\n",
    "    pcn=np.sum(np.where(pred==0,1,0))\n",
    "    cp=np.sum(np.where(ytrue==1,1,0))\n",
    "    cn=np.sum(np.where(ytrue==0,1,0))\n",
    "    ppv=[tn*1.0/pcn, tp*1.0/pcp]\n",
    "    trp=[tn*1.0/cn, tp*1.0/cp]\n",
    "    ac=(tp+tn)*1.0/(cp+cn)\n",
    "    f1=[2.0*ppv[0]*trp[0]/(ppv[0]+trp[0]), 2.0*ppv[1]*trp[1]/(ppv[1]+trp[1])]\n",
    "    #if return_f1:\n",
    "    #    if return_ac:\n",
    "    #        return (f1[0]*cn+f1[1]*cp)/(cn+cp),ac\n",
    "    #    else:\n",
    "    #        return (f1[0]*cn+f1[1]*cp)/(cn+cp)\n",
    "    #if return_ac:\n",
    "    #    return ac\n",
    "    display(\"              precision     recall     f1-score     true number    predicted number\")\n",
    "    print()\n",
    "    print(\"           0      %5.3f      %5.3f        %5.3f        %8i    %16i\"%(ppv[0],trp[0],f1[0],cn,pcn))\n",
    "    print(\"           1      %5.3f      %5.3f        %5.3f        %8i    %16i\"%(ppv[1],trp[1],f1[1],cp,pcp))\n",
    "    print()\n",
    "    print(\"    accuracy                              %5.3f        %8i\"%((tp+tn)*1.0/(cp+cn),cp+cn))\n",
    "    print(\"   macro avg      %5.3f      %5.3f        %5.3f        %8i\"%((ppv[0]+ppv[1])/2.0,(trp[0]+trp[1])/2.0, (f1[0]+f1[1])/2.0,cn+cp))\n",
    "    print(\"weighted avg      %5.3f      %5.3f        %5.3f        %8i\"%((ppv[0]*cn+ppv[1]*cp)/(cn+cp),(trp[0]*cn+trp[1]*cp)/(cn+cp), (f1[0]*cn+f1[1]*cp)/(cn+cp),cn+cp))\n",
    "    \n",
    "    \n",
    "    #the F1-score is simply the harmonic mean of precision (PRE) and recall (REC)\n",
    "\n",
    "    #F1 = 2 * (PRE * REC) / (PRE + REC)\n",
    "    \n",
    "    # If we write the two metrics PRE and REC in terms of \n",
    "    # true positives (TP), true negatives (TN), false positives \n",
    "    # (FP), and false negatives (FN), we get:\n",
    "    \n",
    "    #PRE = TP / (TP + FP)\n",
    "    #REC = TP / (TP + FN)\n",
    "    #Thus, the precision score gives us an idea \n",
    "    #(expressed as a score from 1.0 to 0.0, from good to bad) \n",
    "    #of the proportion of how many actual spam emails (TP) we \n",
    "    #correctly classified as spam among all the emails we classified \n",
    "    #as spam (TP + FP). In contrast, the recall (also ranging from 1.0 to 0.0) \n",
    "    #tells us about how many of the actual spam emails (TP) we \"retrieved\" or \n",
    "    #\"recalled\" (TP + FN).    \n",
    "    '''\n",
    "    1. ROC Curve <= both scikit and manual\n",
    "    2. Scatter Plot diagram of Actual and Predicted probability\n",
    "    3. Table of Area Ratio (test data), Accuracy (Test Data), Accuracy (Average),\n",
    "    F1 score (test data), F1 (score average) <=\n",
    "    4. Plots of Gradient Descent and MiniBatch Gradient Descent Cost functions\n",
    "    for  different epochs\n",
    "    5. plots of ROC and AUC Curves of epochs\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    The corresponding Scikit Learn Alternative. I could've done something like\n",
    "    GridSearch or other function to tune the parameters, but I am already doing \n",
    "    it, by looping over some values.\n",
    "    '''\n",
    "    \n",
    "    return myResult#print(i, j, alpha, lambd)\n",
    "\n",
    "# Feed Forward Neural Network\n",
    "def CallKerasModel(NNArch, nLayers):\n",
    "    classifier = Sequential()\n",
    "    print(\"nHidden\", nHidden)\n",
    "\n",
    "    for layer in range(1, nLayers):\n",
    "        if layer == 0:\n",
    "            classifier.add(Dense(nHidden, activation=NNArch[layer]['AF'], \\\n",
    "                                 kernel_initializer='random_normal', input_dim=nFeatures))\n",
    "        elif layer == nLayers-1:\n",
    "            classifier.add(Dense(nOutput, activation=NNArch[layer]['AF'], \\\n",
    "                                 kernel_initializer='random_normal'))\n",
    "        else:\n",
    "            classifier.add(Dense(nHidden, activation=NNArch[layer]['AF'], \\\n",
    "                                 kernel_initializer='random_normal'))\n",
    "    return classifier\n",
    "\n",
    "'''\n",
    "Main Body of the Program\n",
    "'''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Estimate how much time it took for program to work\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Getting parameter file    \n",
    "    paramFile = 'ParamFile.yaml'\n",
    "    # getting NN configuration - a lot of parameters to trace T_T\n",
    "    dataProc = data_processing.NetworkArchitecture(paramFile)\n",
    "    \n",
    "    with open(paramFile) as f:\n",
    "        paramData = yaml.load(f, Loader = yaml.FullLoader)\n",
    "        \n",
    "    NNType = paramData['type']\n",
    "    outputPath = paramData['outputPath']\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.makedirs(outputPath)\n",
    "    \n",
    "    '''\n",
    "    So, If the total number of layers = 2, we can switch to Simple Logistic regression.\n",
    "    However, once the number of layers is > 2, we are going to use Neural Network.\n",
    "    The number of hidden layers can be specified in the parameter file above \n",
    "    (this is done for simplicity). In principle, the Neural network without hidden layers,\n",
    "    should produc ethe same results as logistic regression (at least, i think so).\n",
    "    '''\n",
    "    acts_hidden = [\"logistic\", \"tanh\", \"relu\"]\n",
    "    act_o = \"identity\"\n",
    "    alphas = np.logspace(-5, 1, 7)\n",
    "    lambdas = np.logspace(-5, 1, 7)\n",
    "    \n",
    "    #for i, eta in enumerate(eta_vals):\n",
    "    #for j, lmbd in enumerate(lmbd_vals):\n",
    "    #    for k, act_h in enumerate(acts_hidden):\n",
    "    #        for l, hn in enumerate(hidden_neurons):\n",
    "    \n",
    "    \n",
    "    nproc = int(paramData['nProc'])\n",
    "    # Checking which Problem we are facing\n",
    "    if (NNType == 'Classification'):\n",
    "        NNType, NNArch, nLayers, nFeatures, \\\n",
    "        nHidden, nOutput, epochs, alpha, lmbd, \\\n",
    "        X, X_train, X_test, Y_train, Y_test, Y_train_onehot, Y_test_onehot,\\\n",
    "        m, nInput, seed, onehotencoder,\\\n",
    "        BatchSize, Optimization = dataProc.CreateNetwork()\n",
    "        # passing data to the function\n",
    "        logisticData = NNType, NNArch, nLayers, nFeatures, \\\n",
    "        nHidden, nOutput, epochs, \\\n",
    "        X, X_train, X_test, Y_train, Y_test, Y_train_onehot, Y_test_onehot,\\\n",
    "        m, nInput, seed, onehotencoder,\\\n",
    "        BatchSize, Optimization\n",
    "        '''\n",
    "        # If we want to explore parameter space, we should get data from \n",
    "        # the parameter file. So, we need to do a Grid Search for the best\n",
    "        parameters. As it is expensive operation\n",
    "        '''\n",
    "        '''\n",
    "        Checking the number of layers. If 2 then it is simple classification\n",
    "        problem. If > 2, we are facing Neural Network. If < 2, you should go\n",
    "        and check parameter file\n",
    "        '''\n",
    "        if (nLayers == 2):        \n",
    "            print('''\n",
    "                  =====================================\n",
    "                  Logistic Regression Via Manual Coding\n",
    "                  =====================================\n",
    "                  Activation Function is:    {}\n",
    "                  No.of test data points:    {}\n",
    "                  No of epochs to learn:     {}\n",
    "                  Learning Rate, \\u03B1:     {}\n",
    "                  Regularization param, \\u03BB: {}\n",
    "                              '''.format('function',\n",
    "                                          X_test.shape[0],\n",
    "                                          epochs,\n",
    "                                          alpha,\n",
    "                                          lmbd)+\n",
    "                  '''\n",
    "                  =====================================\n",
    "                  ''')\n",
    "            #pipeline = regression.RegressionPipeline()\n",
    "            \n",
    "            \n",
    "            # Getting cost function averaged over all epochs\n",
    "            #costs, theta = pipeline.DoLogisticRegression(X_train, \\\n",
    "            #                                            Y_train, epochs, lmbd, alpha)\n",
    "            # getting the resulting values, to plot andto fit later on\n",
    "            #myResult = {alpha: [costs, theta], lmbd: [costs, theta]}\n",
    "            # fitting parameters to the data and evaluate all tests etc.\n",
    "            #Y_pred = pipeline.PredictLogisticRegression(X_test, theta)\n",
    "            #print(np.shape(Y_pred))\n",
    "            #print((Y_train))\n",
    "            #Y_true = Y_train\n",
    "            #own_classification_report\n",
    "            # enabling parallel processing\n",
    "            myResult = Parallel(n_jobs=nproc, verbose=10)(delayed(DoGriSearchClassification)\\\n",
    "             (logisticData, i, j, alpha, lmbd) for i, alpha in enumerate(alphas) for j, lmbd in enumerate(lambdas))\n",
    "            \n",
    "            \n",
    "            #tp=0\n",
    "            #tn=0\n",
    "            #fp=0\n",
    "            #fn=0\n",
    "            #Y_train=np.where(Y_train>0.5,1,0)\n",
    "            #for i in range(len(Y_true)):\n",
    "            #    if (Y_pred[i]==1 and Y_true[i]==1):\n",
    "            #        tp +=1\n",
    "            #    elif (Y_pred[i]==1 and Y_true[i]==0):\n",
    "            #        fp +=1\n",
    "            #    elif (Y_pred[i]==0 and Y_true[i]==0):\n",
    "            #        tn +=1\n",
    "            #    elif (Y_pred[i]==0 and Y_true[i]==1):\n",
    "            #        fn +=1\n",
    "                    \n",
    "            # Accuracy: checking for 0 occurence\n",
    "            #Accuracy = (TP + TN) / float(len(Y_true)) if Y_true else 0\n",
    "            # Precision:\n",
    "            #Precision = TP / (TP + FP)\n",
    "            # Recall (should be above 0.5 than it is good)\n",
    "            #Recall = TP / (TP + FN)\n",
    "            # F1, could've used 2 * (PRE * REC) / (PRE + REC), but this one doesn't suffer\n",
    "            # from 0 devision issue\n",
    "            #F1 = (2 * TP) / (2 * TP + FP + FN)\n",
    "            #PSP = np.sum(np.where(Y_pred==1,1,0))\n",
    "            #PSN = np.sum(np.where(Y_pred==0,1,0))\n",
    "            #SP  = np.sum(np.where(Y_true==1,1,0))\n",
    "            #SN  = np.sum(np.where(Y_true==0,1,0))\n",
    "            #ppv=[tn*1.0/pcn, tp*1.0/pcp]\n",
    "            #trp=[tn*1.0/cn, tp*1.0/cp]\n",
    "            #Accuracy=(TP + TN)*1.0/(SP + SN)\n",
    "            '''\n",
    "            ytrue = Y_train\n",
    "            pred = Y_pred\n",
    "            threshold=0.5\n",
    "            return_f1=False\n",
    "            return_ac=False\n",
    "            tp=0\n",
    "            tn=0\n",
    "            fp=0\n",
    "            fn=0\n",
    "            pred=np.where(pred>threshold,1,0)\n",
    "            for i in range(len(ytrue)):\n",
    "                if (pred[i]==1 and ytrue[i]==1):\n",
    "                    tp +=1\n",
    "                elif (pred[i]==1 and ytrue[i]==0):\n",
    "                    fp +=1\n",
    "                elif (pred[i]==0 and ytrue[i]==0):\n",
    "                    tn +=1\n",
    "                elif (pred[i]==0 and ytrue[i]==1):\n",
    "                    fn +=1\n",
    "            pcp=np.sum(np.where(pred==1,1,0))\n",
    "            pcn=np.sum(np.where(pred==0,1,0))\n",
    "            cp=np.sum(np.where(ytrue==1,1,0))\n",
    "            cn=np.sum(np.where(ytrue==0,1,0))\n",
    "            ppv=[tn*1.0/pcn, tp*1.0/pcp]\n",
    "            trp=[tn*1.0/cn, tp*1.0/cp]\n",
    "            ac=(tp+tn)*1.0/(cp+cn)\n",
    "            f1=[2.0*ppv[0]*trp[0]/(ppv[0]+trp[0]), 2.0*ppv[1]*trp[1]/(ppv[1]+trp[1])]\n",
    "            #if return_f1:\n",
    "            #    if return_ac:\n",
    "            #        return (f1[0]*cn+f1[1]*cp)/(cn+cp),ac\n",
    "            #    else:\n",
    "            #        return (f1[0]*cn+f1[1]*cp)/(cn+cp)\n",
    "            #if return_ac:\n",
    "            #    return ac\n",
    "            print(\"              precision     recall     f1-score     true number    predicted number\")\n",
    "            print()\n",
    "            print(\"           0      %5.3f      %5.3f        %5.3f        %8i    %16i\"%(ppv[0],trp[0],f1[0],cn,pcn))\n",
    "            print(\"           1      %5.3f      %5.3f        %5.3f        %8i    %16i\"%(ppv[1],trp[1],f1[1],cp,pcp))\n",
    "            print()\n",
    "            print(\"    accuracy                              %5.3f        %8i\"%((tp+tn)*1.0/(cp+cn),cp+cn))\n",
    "            print(\"   macro avg      %5.3f      %5.3f        %5.3f        %8i\"%((ppv[0]+ppv[1])/2.0,(trp[0]+trp[1])/2.0, (f1[0]+f1[1])/2.0,cn+cp))\n",
    "            print(\"weighted avg      %5.3f      %5.3f        %5.3f        %8i\"%((ppv[0]*cn+ppv[1]*cp)/(cn+cp),(trp[0]*cn+trp[1]*cp)/(cn+cp), (f1[0]*cn+f1[1]*cp)/(cn+cp),cn+cp))\n",
    "            '''\n",
    "            # Evaluating scores\n",
    "            #Accuracy, Precision, Recall, F1 = funclib.ErrorFuncs.CallF1(Y_train, Y_pred)\n",
    "            #print('nknfsnflibsfksbkfbls')\n",
    "            # adding result to the dictionary\n",
    "            #myResult[alpha] = [Accuracy, Precision, Recall, F1]\n",
    "            #myResult[lmbd] = [Accuracy, Precision, Recall, F1]\n",
    "            # enabling parallel processing\n",
    "            #myResult = Parallel(n_jobs=nproc, verbose=10)(delayed(DoGriSearchClassification)\\\n",
    "            # (logisticData, i, j, alpha, lambd) for i, alpha in enumerate(alphas) for j, lambd in enumerate(lambdas))\n",
    "            print('''\n",
    "                  =====================================\n",
    "                  Logistic Regression Via Scikit Learn\n",
    "                  =====================================\n",
    "                  Activation Function is:    {}\n",
    "                  No.of test data points:    {}\n",
    "                  No of epochs to learn:     {}\n",
    "                  Learning Rate, \\u03B1:     {}\n",
    "                  Regularization param, \\u03BB: {}\n",
    "                              '''.format('function',\n",
    "                                          X_test.shape[0],\n",
    "                                          epochs,\n",
    "                                          alpha,\n",
    "                                          lmbd)+\n",
    "                  '''\n",
    "                  =====================================\n",
    "                  ''')\n",
    "        \n",
    "            XTrain = X_train\n",
    "            yTrain = Y_train\n",
    "            XTest = X_test\n",
    "            yTest = Y_test\n",
    "            print(\"Doing logreg using sklearn\")\n",
    "            #%Setting up grid search for optimal parameters of Logistic regression\n",
    "\n",
    "    \n",
    "            lambdas=np.logspace(-5,7,13)\n",
    "            parameters = [{'C': 1./lambdas, \"solver\":[\"lbfgs\"]}]#*len(parameters)}]\n",
    "            scoring = ['accuracy', 'roc_auc']\n",
    "            logReg = LogisticRegression()\n",
    "            # Finds best hyperparameters, then does regression.\n",
    "            gridSearch = GridSearchCV(logReg, parameters, cv=5, scoring=scoring, refit='roc_auc') \n",
    "    \n",
    "            # Fit stuff\n",
    "            gridSearch.fit(XTrain, yTrain.ravel())\n",
    "            yTrue, yPred = yTest, gridSearch.predict(XTest)\n",
    "            print(classification_report(yTrue,yPred))\n",
    "            rep = pd.DataFrame(classification_report(yTrue,yPred,output_dict=True)).transpose()\n",
    "            display(rep)\n",
    "    \n",
    "            logreg_df = pd.DataFrame(gridSearch.cv_results_) # Shows behaviour of CV\n",
    "            display(logreg_df[['param_C','mean_test_accuracy', 'rank_test_accuracy','mean_test_roc_auc', 'rank_test_roc_auc']])\n",
    "    \n",
    "            logreg_df.columns\n",
    "            logreg_df.plot(x='param_C', y='mean_test_accuracy', yerr='std_test_accuracy', logx=True)\n",
    "            logreg_df.plot(x='param_C', y='mean_test_roc_auc', yerr='std_test_roc_auc', logx=True)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            print(myResult)\n",
    "            '''\n",
    "            Plotting Results - Cost Function\n",
    "            '''\n",
    "            epochs1 = range(epochs)\n",
    "            # Plotting results\n",
    "            fig,ax = plt.subplots(figsize=(12,8))\n",
    "        \n",
    "            ax.set_ylabel('J(Theta)')\n",
    "            ax.set_xlabel('Iterations')\n",
    "            ax.plot(epochs1, myResult['costs'],'b.')\n",
    "            \n",
    "            '''\n",
    "            m = np.size(Y_train)\n",
    "            neuralNet = neural.NeuralNetwork(NNType, NNArch, \\\n",
    "                                             nLayers, nFeatures, \\\n",
    "                                             nHidden, nOutput, \\\n",
    "                                             epochs, alpha, \\\n",
    "                                             lmbd, nInput, seed)\n",
    "            modelParams, costs = neuralNet.TrainNetwork(X_train, Y_train, m)\n",
    "            '''\n",
    "            \n",
    "        '''\n",
    "        Doing Neural Networks if the amount of layeres is more than 2\n",
    "        '''\n",
    "            \n",
    "        if (nLayers > 2):\n",
    "            \n",
    "            '''\n",
    "            Switching to Neural Network\n",
    "            '''\n",
    "            \n",
    "            m = np.size(Y_train)\n",
    "            print('Neural Network')\n",
    "            # passing configuration\n",
    "            neuralNet = neural.NeuralNetwork(NNType, NNArch, \\\n",
    "                                             nLayers, nFeatures, \\\n",
    "                                             nHidden, nOutput, \\\n",
    "                                             epochs, alpha, \\\n",
    "                                             lmbd, nInput, seed, BatchSize)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            1. Add Parallelisation\n",
    "            2. Add plots\n",
    "            3. Save all the values for scores etc. to table or whatever\n",
    "            '''\n",
    "            #myResult = Parallel(n_jobs=nproc, verbose=10)(delayed(DoGriSearchClassification)\\\n",
    "            # (logisticData, i, j, alpha, lmbd) for i, alpha in enumerate(alphas) for j, lmbd in enumerate(lambdas))\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(type(BatchSize))\n",
    "            if (BatchSize==0):\n",
    "                print(\"Gradient Descent\")\n",
    "                modelParams, costs = neuralNet.TrainNetworkGD(X_train, Y_train_onehot, m)\n",
    "                print('Y_train_onehot', np.shape(Y_train_onehot))\n",
    "                print('Y_train_', np.shape(Y_train))\n",
    "                #print(\"X_train\", \"Y_train_onehot\")\n",
    "                test_predict = neuralNet.MakePrediction(X, modelParams)\n",
    "                print('test_predict',np.shape(Y_test_onehot), np.shape(Y_test))\n",
    "                #print(Y_test_onehot)\n",
    "                #print(test_predict)\n",
    "                print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "            elif (BatchSize > 0):\n",
    "                print(\"Mini Batches\")\n",
    "                modelParams, costs = neuralNet.TrainNetworkMBGD(X_train, Y_train_onehot, m)#TrainNetworkMBGD(X_train, Y_train_onehot, m)\n",
    "                test_predict = neuralNet.MakePrediction(X, modelParams)\n",
    "                print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "            \n",
    "            \n",
    "            #modelParams, \n",
    "            #print(nHidden)\n",
    "            # Classify using sklearn\n",
    "            clf = MLPClassifier(solver=\"lbfgs\", alpha=alpha, hidden_layer_sizes=nHidden)\n",
    "            clf.fit(X_train, Y_train)\n",
    "            yTrue, yPred = Y_test, clf.predict(X_test)\n",
    "            print(classification_report(yTrue, yPred))\n",
    "            print(\"Roc auc: \", roc_auc_score(yTrue, yPred))\n",
    "            \n",
    "            print('''\n",
    "                  Initialising Keras\n",
    "                  ''')\n",
    "            # decoding from keras\n",
    "            #print(np.shape(Y_train))\n",
    "            #Y_train = np.argmax(Y_train, axis=1)#.reshape(1,-1)\n",
    "            #print(np.shape(Y_train))\n",
    "            \n",
    "            classifier = CallKerasModel(NNArch, nLayers)\n",
    "            \n",
    "            '''\n",
    "            To optimize our neural network we use Adam. Adam stands for Adaptive \n",
    "            moment estimation. Adam is a combination of RMSProp + Momentum.\n",
    "            '''\n",
    "            #print(np.shape(Y_train))\n",
    "            #Y_train = Y_train.reshape(1,-1)\n",
    "            #decoded = Y_train.dot(onehotencoder.active_features_).astype(int)\n",
    "            # invert the one hot encoded data\n",
    "            #inverted = onehotencoder.inverse_transform([argmax(Y_train[:, :])])\n",
    "            #print(Y_train)\n",
    "            #Y_train = Y_train.reshape(-1,1)\n",
    "            #print(inverted)\n",
    "            if BatchSize == 0:\n",
    "                BatchSize = 100\n",
    "            # Stochatic gradient descent\n",
    "            sgd = SGD(lr=alpha)\n",
    "            classifier.compile(optimizer = sgd, loss='binary_crossentropy', metrics =['accuracy'])\n",
    "            #Fitting the data to the training dataset\n",
    "            classifier.fit(X_train, Y_train_onehot, batch_size=BatchSize, epochs = epochs)\n",
    "            \n",
    "            \n",
    "            #def build_model(hidden_layer_sizes):\n",
    "            #  model = Sequential()\n",
    "            \n",
    "            #  model.add(Dense(hidden_layer_sizes[0], input_dim=2))\n",
    "            #  model.add(Activation('tanh'))\n",
    "            \n",
    "            #  for layer_size in hidden_layer_sizes[1:]:\n",
    "            #    model.add(Dense(layer_size))\n",
    "            #    model.add(Activation('tanh'))\n",
    "            \n",
    "            #  model.add(Dense(1))\n",
    "            #  model.add(Activation('sigmoid'))\n",
    "            \n",
    "            #  return model\n",
    "            \n",
    "            #def build_model(hidden_layer_sizes):\n",
    "            #  model = Sequential()\n",
    "            \n",
    "            #  model.add(Dense(hidden_layer_sizes[0], input_dim=2))\n",
    "            #  model.add(Activation('tanh'))\n",
    "            \n",
    "            #  for layer_size in hidden_layer_sizes[1:]:\n",
    "            #    model.add(Dense(layer_size))\n",
    "            #    model.add(Activation('tanh'))\n",
    "            \n",
    "            #  model.add(Dense(1))\n",
    "            #  model.add(Activation('sigmoid'))\n",
    "            \n",
    "            #  return model\n",
    "            \n",
    "            #max_epochs = 500\n",
    "            #my_logger = MyLogger(n=50)\n",
    "            #h = model.fit(train_x, train_y, batch_size=32, epochs=max_epochs, verbose=0, callbacks=[my_logger])\n",
    "            \n",
    "            np.set_printoptions(precision=4, suppress=True)\n",
    "            eval_results = classifier.evaluate(X_test, Y_test_onehot, verbose=0) \n",
    "            print(\"\\nLoss, accuracy on test data: \")\n",
    "            print(\"%0.4f %0.2f%%\" % (eval_results[0], eval_results[1]*100))\n",
    "            \n",
    "        else:\n",
    "            '''\n",
    "            Raise an exception, if number of layers is smaller than 2. It shouldn't be the case,\n",
    "            because in param file I am specifying number of hidden layers and not the total layers.\n",
    "            Then I add 2 to that number in the code. But better safe than sorry :) \n",
    "            '''\n",
    "            raise Exception('No. of Layers should be >= {}! Check Parameter File.'.format(nLayers))\n",
    "    \n",
    "        \n",
    "    elif (NNType == 'Regression'):\n",
    "        NNType, NNArch, nLayers, \\\n",
    "        nFeatures, nHidden, nOutput, \\\n",
    "        epochs, alpha, lmbd, X, X_train, \\\n",
    "        X_test, Y_train, Y_test, m,\\\n",
    "        x, y, z,\\\n",
    "        x_rav, y_rav, z_rav, zshape,\\\n",
    "        nInput, seed, BatchSize, Optimization = dataProc.CreateNetwork()\n",
    "        '''\n",
    "        Polynomial Regression on Franke Function\n",
    "        '''\n",
    "        if (nLayers == 2):        \n",
    "            print('''\n",
    "                  =====================================\n",
    "                   Linear Regression Via Manual Coding\n",
    "                  =====================================\n",
    "                  Activation Function is:    \n",
    "                  No.of test data points:    \n",
    "                  No of epochs to learn:     \n",
    "                  Learning Rate, \\u03B1:     \n",
    "                  Regularization param, \\u03BB: \n",
    "                              '''#.format('function',\n",
    "                                 #         X_test.shape[0],\n",
    "                                 #         epochs,\n",
    "                                 #         alpha,\n",
    "                                 #         lmbd)+\n",
    "                  '''\n",
    "                  =====================================\n",
    "                  ''')\n",
    "            #dataProc.ProcessData()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            print('''\n",
    "                  =====================================\n",
    "                   Linear Regression Via Scikit Learn\n",
    "                  =====================================\n",
    "                  Activation Function is:    \n",
    "                  No.of test data points:    \n",
    "                  No of epochs to learn:     \n",
    "                  Learning Rate, \\u03B1:     \n",
    "                  Regularization param, \\u03BB: \n",
    "                              '''#.format('function',\n",
    "                                 #         X_test.shape[0],\n",
    "                                 #         epochs,\n",
    "                                 #         alpha,\n",
    "                                 #         lmbd)+\n",
    "                  '''\n",
    "                  =====================================\n",
    "                  ''')\n",
    "                \n",
    "        # If layer more than 2, we are using Neural Network\n",
    "        if (nLayers > 2): \n",
    "            '''\n",
    "            Switching to Neural Network\n",
    "            '''\n",
    "            \n",
    "            m = np.size(Y_train)\n",
    "            print('Neural Network')\n",
    "            # passing configuration\n",
    "            neuralNet = neural.NeuralNetwork(NNType, NNArch, \\\n",
    "                                             nLayers, nFeatures, \\\n",
    "                                             nHidden, nOutput, \\\n",
    "                                             epochs, alpha, \\\n",
    "                                             lmbd, nInput, seed, BatchSize)\n",
    "            \n",
    "            #print(type(BatchSize))\n",
    "            print(X_train)\n",
    "            if (BatchSize==0):\n",
    "                print(\"Gradient Descent\")\n",
    "                modelParams, costs = neuralNet.TrainNetworkGD(X_train, Y_train, m)\n",
    "                print(modelParams)\n",
    "                #print(modelParams)\n",
    "                \n",
    "                #print('Y_train_onehot', np.shape(Y_train))\n",
    "                #print('Y_train_', np.shape(Y_train))\n",
    "                #print(\"X_train\", \"Y_train_onehot\")\n",
    "                \n",
    "\n",
    "                \n",
    "                Y_test_pred = neuralNet.MakePrediction(X_test, modelParams)\n",
    "                \n",
    "                \n",
    "                #print(Y_test_pred)\n",
    "                \n",
    "                #print('test_predict',np.shape(Y_test))\n",
    "                #print(Y_test_onehot)\n",
    "                #print(test_predict)\n",
    "                #print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "                #x = x_rav\n",
    "                #y = y_rav\n",
    "                #print(\"X is\", X.shape)\n",
    "                Y_pred = neuralNet.MakePrediction(X, modelParams)\n",
    "                \n",
    "                print(\"Y_pred is \", Y_pred)\n",
    "                \n",
    "                z = Y_pred.reshape(zshape)\n",
    "                print(\"x is\", x.shape)\n",
    "                print(\"y is\", y.shape)\n",
    "                print(\"z is\", z.shape)\n",
    "                # takes an array of z values\n",
    "                #zarray = Y_test_pred\n",
    "                # output dir\n",
    "                #output_dir = args[3]\n",
    "                # filename\n",
    "                #filename = args[4]\n",
    "                #print(filename)\n",
    "                # Turning interactive mode on\n",
    "                #plt.ion()\n",
    "                fig = plt.figure()\n",
    "                axe = fig.add_subplot(1,1,1, projection = '3d')\n",
    "                #axe.view_init(5,50)\n",
    "                #axes = [fig.add_subplot(1, 3, i, projection='3d') for i in range(1, len(zarray) + 1)]\n",
    "                #axes[0].view_init(5,50)\n",
    "                #axes[1].view_init(5,50)\n",
    "                #axes[2].view_init(5,50)\n",
    "                surf = axe.plot_surface(x, y, z, alpha = 0.5,\\\n",
    "                                         cmap = 'brg_r', label=\"Franke function\", linewidth = 0, antialiased = False)\n",
    "        \n",
    "                plt.show()\n",
    "            elif (BatchSize > 0):\n",
    "                print(\"Mini Batches\")\n",
    "                modelParams, costs = neuralNet.TrainNetworkMBGD(X_train, Y_train, m)#TrainNetworkMBGD(X_train, Y_train_onehot, m)\n",
    "                Y_test_pred = neuralNet.MakePrediction(X_test, modelParams)\n",
    "                #print('test_predict',np.shape(Y_test))\n",
    "                #print(Y_test_onehot)\n",
    "                #print(test_predict)\n",
    "                #print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "                #x = x_rav\n",
    "                #y = y_rav\n",
    "                print(\"X is\", X)\n",
    "                Y_pred = neuralNet.MakePrediction(X, modelParams)\n",
    "                \n",
    "                print(\"Y_pred is \", Y_pred)\n",
    "                \n",
    "                z = Y_pred.reshape(zshape)\n",
    "                print(\"x is\", x.shape)\n",
    "                print(\"y is\", y.shape)\n",
    "                print(\"z is\", z.shape)\n",
    "                # takes an array of z values\n",
    "                #zarray = Y_test_pred\n",
    "                # output dir\n",
    "                #output_dir = args[3]\n",
    "                # filename\n",
    "                #filename = args[4]\n",
    "                #print(filename)\n",
    "                # Turning interactive mode on\n",
    "                #plt.ion()\n",
    "                fig = plt.figure(figsize=(10, 5))\n",
    "                axe = fig.add_subplot(1,1,1, projection = '3d')\n",
    "                #axe.view_init(5,50)\n",
    "                #axes = [fig.add_subplot(1, 3, i, projection='3d') for i in range(1, len(zarray) + 1)]\n",
    "                #axes[0].view_init(5,50)\n",
    "                #axes[1].view_init(5,50)\n",
    "                #axes[2].view_init(5,50)\n",
    "                surf = axe.plot_surface(x, y, z, alpha = 0.5,\\\n",
    "                                         cmap = 'brg_r', label=\"Franke function\", linewidth = 0, antialiased = False)\n",
    "        \n",
    "                plt.show()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Plotting Results\n",
    "    '''\n",
    "    #epochs1 = range(epochs)\n",
    "    # Plotting results\n",
    "    #fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    #ax.set_ylabel('J(Theta)')\n",
    "    #ax.set_xlabel('Iterations')\n",
    "    #ax.plot(epochs1, costs,'b.')\n",
    "    \n",
    "    # End time of the program\n",
    "    endTime = time.time()\n",
    "    print(\"-- Program finished at %s sec --\" % (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
